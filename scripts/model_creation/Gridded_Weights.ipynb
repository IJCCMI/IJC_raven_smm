{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8RIHiGRQeVe"
      },
      "outputs": [],
      "source": [
        "# if using Google Colab - to connect to drive\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/google_drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install netcdf4 &> /dev/null\n",
        "\n",
        "# load necessary libraries\n",
        "import os\n",
        "import xarray as xr\n",
        "import geopandas as gpd\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "yPuZsvWvRAbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# format HRU and subbasin shapefile\n",
        "# HRU shapefile\n",
        "shp_file = gpd.read_file('/content/google_drive/MyDrive/Milk_Mary_model/shapefile/smm_hrus_filled_R1.shp')\n",
        "\n",
        "# subbasin\n",
        "sub_file = gpd.read_file('/content/google_drive/MyDrive/Milk_Mary_model/shapefile/smm_subbasins_R1.shp')\n",
        "sub_file_rename = sub_file.rename(columns ={'ID':'SUB_ID'})\n",
        "sub_file_up = sub_file_rename.drop('geometry',1)\n",
        "\n",
        "# merge based on subbasin ID\n",
        "df1 = shp_file.merge(sub_file_up, how='left', on='SUB_ID')\n",
        "\n",
        "# export\n",
        "shp_file_up = gpd.GeoDataFrame(df1, geometry='geometry')\n",
        "shp_file_up_rename = shp_file_up.rename(columns ={'SUB_ID':'SubId'})\n",
        "shp_file_up_rename1 = shp_file_up_rename.rename(columns ={'DS_ID':'DowSubId'})\n",
        "shp_file_up_rename2 = shp_file_up_rename1.rename(columns ={'Area':'HRU_Area'})\n",
        "shp_file_up_final = shp_file_up_rename2.rename(columns ={'Area_km2':'BasArea'})\n",
        "shp_file_up_final.plot()\n",
        "shp_file_up_final.to_file('/content/smm_hrus_updated.shp')"
      ],
      "metadata": {
        "id": "i61GMdomQkMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define NetCDF File Path\n",
        "# -i [your-nc-file]\n",
        "# filename of NetCDF file or shapefile that contains how you discretized your model\n",
        "\n",
        "forcing_var = \"/content/google_drive/MyDrive/Milk_Mary_model/workflow_outputs/RavenInput/input/mean_prcp_daily.nc\""
      ],
      "metadata": {
        "id": "PhIAE-AMQjf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optional - uncomment to get an overview of the CaSPAr data\n",
        "check_casp = xr.open_dataset(os.path.join(forcing_var))\n",
        "#print(check_casp)\n",
        "#display(check_casp)  # available if in Jupyter notebook"
      ],
      "metadata": {
        "id": "2tQRbNJFQkE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Dimension and Variable Names\n",
        "# -d [dim-names]\n",
        "# names of NetCDF dimensions of longitude (x) and latitude (y) in this order, e.g. \"rlon,rlat\"<br>\n",
        "# if you are using RDRS data, input: \"rlon,rlat\"\n",
        "\n",
        "dim_names = \"rlon,rlat\""
      ],
      "metadata": {
        "id": "mZE9cZBdQkHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -v [var-names]\n",
        "# names of 2D NetCDF variables containing longitudes and latitudes (in this order) of centroids of grid cells, e.g. \"lon,lat\"<br>\n",
        "# if you are using RDRS data, input: \"lon,lat\"\n",
        "\n",
        "var_names = \"lon,lat\""
      ],
      "metadata": {
        "id": "wWIq0qjEQkKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define HRU Shapefile Path\n",
        "# -r [tlbx-shp-file]**<br>\n",
        "# name of shapefile routing toolbox provides; shapefile contains shapes of all land and lake HRUs,e.g. \"HRUs.shp\"<br>\n",
        "\n",
        "boundary_shp = \"/content/smm_hrus_updated.shp\""
      ],
      "metadata": {
        "id": "YhyCIx1rbHD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Subbasin ID OR Gauge Flow ID\n",
        "\n",
        "#-s [subbasin-id]\n",
        "# (**either** -s [subbasin-id] or -b [gauge-id] must be set)\n",
        "# ID of subbasin  most downstream (likely a subbasin that contains a streamflow\n",
        "#gauge station but can be any subbasin ID); script will include all subbasins upstream of the given subbasin automatically;\n",
        "# according attribute in [tlbx-shp-file] is called \"SubId\"; e.g. \"7202\"<br>\n",
        "# Refer to 1.1_Shapefile.ipynb to identify subbasin\n",
        "subId = \"113656\"\n",
        "\n",
        "# if you rather define -b [gauge-id], input subID as NA_\n",
        "if subId == \"NA\":\n",
        "  subId = None\n",
        "\n",
        "#-b [gauge-id]\n",
        "#(**either** -b [gauge-id] or -s [subbasin-id] must be set)</font> ID of streamflow gauging station;\n",
        "#according attribute in [tlbx-shp-file] is called \"Obs_NM\"; e.g. \"02LE024\n",
        "gaugeId = \"NA\"\n",
        "# if you rather define -b [gauge-id], input subID as NA_\n",
        "\n",
        "if gaugeId == \"NA\":\n",
        "  gaugeId = None"
      ],
      "metadata": {
        "id": "OZGpxbA4e_vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Output Name\n",
        "# -o [output-name]\n",
        "# define output filename\n",
        "\n",
        "output_name = \"GridWeights2.txt\""
      ],
      "metadata": {
        "id": "3X5ZRnJ1f36Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run gridded weight generator\n",
        "#!/usr/bin/env python\n",
        "from __future__ import print_function\n",
        "\n",
        "# Copyright 2016-2020 Juliane Mai - juliane.mai(at)uwaterloo.ca\n",
        "\n",
        "# read netCDF files\n",
        "import netCDF4 as nc4\n",
        "\n",
        "# command line arguments\n",
        "import argparse\n",
        "\n",
        "# checking file paths and file extensions\n",
        "from pathlib import Path\n",
        "\n",
        "# to perform numerics\n",
        "import numpy as np\n",
        "\n",
        "# read shapefiles and convert to GeoJSON and WKT\n",
        "import geopandas as gpd\n",
        "\n",
        "# get equal-area projection and derive overlay\n",
        "from   osgeo   import ogr\n",
        "from   osgeo   import osr\n",
        "from   osgeo   import __version__ as osgeo_version\n",
        "\n",
        "\n",
        "\n",
        "input_file           = forcing_var\n",
        "dimname              = dim_names\n",
        "varname              = var_names\n",
        "routinginfo          = boundary_shp\n",
        "basin                = gaugeId  # e.g. \"02LE024\"\n",
        "SubId                = subId  # e.g. 7202\n",
        "output_file          = output_name\n",
        "doall                = False\n",
        "key_colname          = \"HRU_ID\"\n",
        "key_colname_model    = None\n",
        "area_error_threshold = 0.05\n",
        "dojson               = False\n",
        "\n",
        "parser      = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,\n",
        "              description='''Convert files from ArcGIS raster format into NetDF file usable in CaSPAr.''')\n",
        "parser.add_argument('-i', '--input_file', action='store',\n",
        "                    default=input_file, dest='input_file', metavar='input_file',\n",
        "                    help='Either (A) Example NetCDF file containing at least 1D or 2D latitudes and 1D or 2D longitudes where grid needs to be representative of model outputs that are then required to be routed. Or (B) a shapefile that contains shapes of subbasins and one attribute that is defining its index in the NetCDF model output file (numbering needs to be [0 ... N-1]).')\n",
        "parser.add_argument('-d', '--dimname', action='store',\n",
        "                    default=dimname, dest='dimname', metavar='dimname',\n",
        "                    help='Dimension names of longitude (x) and latitude (y) (in this order). Example: \"rlon,rlat\", or \"x,y\"')\n",
        "parser.add_argument('-v', '--varname', action='store',\n",
        "                    default=varname, dest='varname', metavar='varname',\n",
        "                    help='Variable name of 2D longitude and latitude variables in NetCDF (in this order). Example: \"lon,lat\".')\n",
        "parser.add_argument('-r', '--routinginfo', action='store',\n",
        "                    default=routinginfo, dest='routinginfo', metavar='routinginfo',\n",
        "                    help='Shapefile that contains all information for the catchment of interest (and maybe some more catchments).')\n",
        "parser.add_argument('-b', '--basin', action='store',\n",
        "                    default=basin, dest='basin', metavar='basin',\n",
        "                    help='Basin of interest (corresponds to \"Gauge_ID\" in shapefile given with -r). Either this or SubId ID (-s) needs to be given. Can be a comma-separated list of basins, e.g., \"02LB005,02LB008\".')\n",
        "parser.add_argument('-s', '--SubId', action='store',\n",
        "                    default=SubId, dest='SubId', metavar='SubId',\n",
        "                    help='SubId of most downstream subbasin (containing usually a gauge station) (corresponds to \"SubId\" in shapefile given with -r). Either this or basin ID (-b) needs to be given. Can be a comma-separated list of SubIds, e.g., \"7399,7400\".')\n",
        "parser.add_argument('-o', '--output_file', action='store',\n",
        "                    default=output_file, dest='output_file', metavar='output_file',\n",
        "                    help='File that will contain grid weights for Raven.')\n",
        "parser.add_argument('-a', '--doall', action='store_true',\n",
        "                    default=doall, dest='doall',\n",
        "                    help='If given, all HRUs found in shapefile are processed. Overwrites settings of \"-b\" and \"-s\". Default: not set (False).')\n",
        "parser.add_argument('-c', '--key_colname', action='store',\n",
        "                    default=key_colname, dest='key_colname', metavar='key_colname',\n",
        "                    help='Name of column in shapefile containing unique key for each dataset. This key will be used in output file. This setting is only used if \"-a\" option is used. \"Default: \"HRU_ID\".')\n",
        "parser.add_argument('-f', '--key_colname_model', action='store',\n",
        "                    default=key_colname_model, dest='key_colname_model', metavar='key_colname_model',\n",
        "                    help='Attribute name in input_file shapefile (option -i) that defines the index of the shape in NetCDF model output file (numbering needs to be [0 ... N-1]). Example: \"NetCDF_col\".')\n",
        "parser.add_argument('-e', '--area_error_threshold', action='store',\n",
        "                    default=area_error_threshold, dest='area_error_threshold', metavar='area_error_threshold',\n",
        "                    help='Threshold (as fraction) of allowed mismatch in areas between subbasins from shapefile (-r) and overlay with grid-cells or subbasins (-i). If error is smaller than this threshold the weights will be adjusted such that they sum up to exactly 1. Raven will exit gracefully in case weights do not sum up to at least 0.95. Default: 0.05.')\n",
        "parser.add_argument('-j', '--dojson', action='store_true',\n",
        "                    default=dojson, dest='dojson',\n",
        "                    help='If given, the GeoJSON of grid cells contributing to at least one HRU are dumped into  GeoJSON. Default: False.')\n",
        "\n",
        "args                 = parser.parse_args()\n",
        "input_file           = args.input_file\n",
        "dimname              = np.array(args.dimname.split(','))\n",
        "varname              = np.array(args.varname.split(','))\n",
        "routinginfo          = args.routinginfo\n",
        "basin                = args.basin\n",
        "SubId                = args.SubId\n",
        "output_file          = args.output_file\n",
        "doall                = args.doall\n",
        "key_colname          = args.key_colname\n",
        "key_colname_model    = args.key_colname_model\n",
        "area_error_threshold = float(args.area_error_threshold)\n",
        "dojson               = args.dojson\n",
        "\n",
        "if dojson:\n",
        "    # write geoJSON files (eventually)\n",
        "    import geojson as gjs\n",
        "\n",
        "if not(SubId is None):\n",
        "\n",
        "    SubId = [ np.int(ss.strip()) for ss in SubId.split(',') ]\n",
        "\n",
        "if (SubId is None) and (basin is None) and not(doall):\n",
        "    raise ValueError(\"Either gauge ID (option -b; e.g., 02AB003) or SubId ID (option -s; e.g., 7173) specified in shapefile needs to be given. You specified none. This basin will be the most downstream gridweights of all upstream subbasins will be added automatically.\")\n",
        "\n",
        "if ( not(SubId is None) ) and ( not(basin is None) ) and not(doall):\n",
        "    raise ValueError(\"Either gauge ID (option -b; e.g., 02AB003) or SubId ID (option -s; e.g., 7173) specified in shapefile needs to be specified. You specified both. This basin will be the most downstream gridweights of all upstream subbasins will be added automatically.\")\n",
        "\n",
        "if not(doall):\n",
        "    key_colname = \"HRU_ID\"    # overwrite any settimg made for this column name in case doall is not set\n",
        "\n",
        "del parser, args\n",
        "\n",
        "\n",
        "# better dont chnage that ever\n",
        "crs_lldeg = 4326        # EPSG id of lat/lon (deg) coordinate referenence system (CRS)\n",
        "crs_caea  = 3573        # EPSG id of equal-area    coordinate referenence system (CRS)\n",
        "\n",
        "\n",
        "def create_gridcells_from_centers(lat, lon):\n",
        "\n",
        "    # create array of edges where (x,y) are always center cells\n",
        "    nlon = np.shape(lon)[1]\n",
        "    nlat = np.shape(lat)[0]\n",
        "    lonh = np.empty((nlat+1,nlon+1), dtype=float)\n",
        "    lath = np.empty((nlat+1,nlon+1), dtype=float)\n",
        "    tmp1 = [ [ (lat[ii+1,jj+1]-lat[ii,jj])/2 for jj in range(nlon-1) ] + [ (lat[ii+1,nlon-1]-lat[ii,nlon-2])/2 ] for ii in range(nlat-1) ]\n",
        "    tmp2 = [ [ (lon[ii+1,jj+1]-lon[ii,jj])/2 for jj in range(nlon-1) ] + [ (lon[ii+1,nlon-1]-lon[ii,nlon-2])/2 ] for ii in range(nlat-1) ]\n",
        "    dlat = np.array(tmp1 + [ tmp1[-1] ])\n",
        "    dlon = np.array(tmp2 + [ tmp2[-1] ])\n",
        "    lonh[0:nlat,0:nlon] = lon - dlon\n",
        "    lath[0:nlat,0:nlon] = lat - dlat\n",
        "\n",
        "    # make lat and lon one column and row wider such that all\n",
        "    lonh[nlat,0:nlon] = lonh[nlat-1,0:nlon] + (lonh[nlat-1,0:nlon] - lonh[nlat-2,0:nlon])\n",
        "    lath[nlat,0:nlon] = lath[nlat-1,0:nlon] + (lath[nlat-1,0:nlon] - lath[nlat-2,0:nlon])\n",
        "    lonh[0:nlat,nlon] = lonh[0:nlat,nlon-1] + (lonh[0:nlat,nlon-1] - lonh[0:nlat,nlon-2])\n",
        "    lath[0:nlat,nlon] = lath[0:nlat,nlon-1] + (lath[0:nlat,nlon-1] - lath[0:nlat,nlon-2])\n",
        "    lonh[nlat,nlon]   = lonh[nlat-1,nlon-1] + (lonh[nlat-1,nlon-1] - lonh[nlat-2,nlon-2])\n",
        "    lath[nlat,nlon]   = lath[nlat-1,nlon-1] + (lath[nlat-1,nlon-1] - lath[nlat-2,nlon-2])\n",
        "\n",
        "    return [lath,lonh]\n",
        "\n",
        "def shape_to_geometry(shape_from_jsonfile,epsg=None):\n",
        "\n",
        "    # converts shape read from shapefile to geometry\n",
        "    # epsg :: integer EPSG code\n",
        "\n",
        "    ring_shape = ogr.Geometry(ogr.wkbLinearRing)\n",
        "\n",
        "    for ii in shape_from_jsonfile:\n",
        "        ring_shape.AddPoint_2D(ii[0],ii[1])\n",
        "    # close ring\n",
        "    ring_shape.AddPoint_2D(shape_from_jsonfile[0][0],shape_from_jsonfile[0][1])\n",
        "\n",
        "    poly_shape = ogr.Geometry(ogr.wkbPolygon)\n",
        "    poly_shape.AddGeometry(ring_shape)\n",
        "\n",
        "    if not( epsg is None):\n",
        "        source = osr.SpatialReference()\n",
        "        source.ImportFromEPSG(crs_lldeg)       # usual lat/lon projection\n",
        "\n",
        "        target = osr.SpatialReference()\n",
        "        target.ImportFromEPSG(epsg)       # any projection to convert to\n",
        "\n",
        "        transform = osr.CoordinateTransformation(source, target)\n",
        "        poly_shape.Transform(transform)\n",
        "\n",
        "    return poly_shape\n",
        "\n",
        "def check_proximity_of_envelops(gridcell_envelop, shape_envelop):\n",
        "\n",
        "    # checks if two envelops are in proximity (intersect)\n",
        "\n",
        "    # minX  --> env[0]\n",
        "    # maxX  --> env[1]\n",
        "    # minY  --> env[2]\n",
        "    # maxY  --> env[3]\n",
        "\n",
        "    if  ((gridcell_envelop[0] <= shape_envelop[1]) and (gridcell_envelop[1] >= shape_envelop[0]) and\n",
        "         (gridcell_envelop[2] <= shape_envelop[3]) and (gridcell_envelop[3] >= shape_envelop[2])):\n",
        "\n",
        "        grid_is_close = True\n",
        "\n",
        "    else:\n",
        "\n",
        "        grid_is_close = False\n",
        "\n",
        "    return grid_is_close\n",
        "\n",
        "def check_gridcell_in_proximity_of_shape(gridcell_edges, shape_from_jsonfile):\n",
        "\n",
        "    # checks if a grid cell falls into the bounding box of the shape\n",
        "    # does not mean it intersects but it is a quick and cheap way to\n",
        "    # determine cells that might intersect\n",
        "\n",
        "    # gridcell_edges = [(lon1,lat1),(lon2,lat2),(lon3,lat3),(lon4,lat4)]\n",
        "    # shape_from_jsonfile\n",
        "\n",
        "    min_lat_cell  = np.min([ii[1] for ii in gridcell_edges])\n",
        "    max_lat_cell  = np.max([ii[1] for ii in gridcell_edges])\n",
        "    min_lon_cell  = np.min([ii[0] for ii in gridcell_edges])\n",
        "    max_lon_cell  = np.max([ii[0] for ii in gridcell_edges])\n",
        "\n",
        "    lat_shape = np.array([ icoord[1] for icoord in shape_from_jsonfile ])     # is it lat???\n",
        "    lon_shape = np.array([ icoord[0] for icoord in shape_from_jsonfile ])     # is it lon???\n",
        "\n",
        "    min_lat_shape = np.min(lat_shape)\n",
        "    max_lat_shape = np.max(lat_shape)\n",
        "    min_lon_shape = np.min(lon_shape)\n",
        "    max_lon_shape = np.max(lon_shape)\n",
        "\n",
        "    if  ((min_lat_cell <= max_lat_shape) and (max_lat_cell >= min_lat_shape) and\n",
        "         (min_lon_cell <= max_lon_shape) and (max_lon_cell >= min_lon_shape)):\n",
        "\n",
        "        grid_is_close = True\n",
        "\n",
        "    else:\n",
        "\n",
        "        grid_is_close = False\n",
        "\n",
        "    return grid_is_close\n",
        "\n",
        "\n",
        "def derive_2D_coordinates(lat_1D, lon_1D):\n",
        "\n",
        "    # nlat = np.shape(lat_1D)[0]\n",
        "    # nlon = np.shape(lon_1D)[0]\n",
        "\n",
        "    # lon_2D =              np.array([ lon_1D for ilat in range(nlat) ], dtype=float32)\n",
        "    # lat_2D = np.transpose(np.array([ lat_1D for ilon in range(nlon) ], dtype=float32))\n",
        "\n",
        "    lon_2D =              np.tile(lon_1D, (lat_1D.size, 1))\n",
        "    lat_2D = np.transpose(np.tile(lat_1D, (lon_1D.size, 1)))\n",
        "\n",
        "    return lat_2D, lon_2D\n",
        "\n",
        "\n",
        "if ( Path(input_file).suffix == '.nc'):\n",
        "\n",
        "    # -------------------------------\n",
        "    # Read NetCDF\n",
        "    # -------------------------------\n",
        "    print(' ')\n",
        "    print('   (1) Reading NetCDF (grid) data ...')\n",
        "\n",
        "    nc_in = nc4.Dataset(input_file, \"r\")\n",
        "    lon      = nc_in.variables[varname[0]][:]\n",
        "    lon_dims = nc_in.variables[varname[0]].dimensions\n",
        "    lat      = nc_in.variables[varname[1]][:]\n",
        "    lat_dims = nc_in.variables[varname[1]].dimensions\n",
        "    nc_in.close()\n",
        "\n",
        "\n",
        "    if len(lon_dims) == 1 and len(lat_dims) == 1:\n",
        "\n",
        "        # in case coordinates are only 1D (regular grid), derive 2D variables\n",
        "\n",
        "        print('   >>> Generate 2D lat and lon fields. Given ones are 1D.')\n",
        "\n",
        "        lat, lon = derive_2D_coordinates(lat,lon)\n",
        "        lon_dims_2D = lat_dims + lon_dims\n",
        "        lat_dims_2D = lat_dims + lon_dims\n",
        "        lon_dims = lon_dims_2D\n",
        "        lat_dims = lat_dims_2D\n",
        "\n",
        "    elif len(lon_dims) == 2 and len(lat_dims) == 2:\n",
        "\n",
        "        # Raven numbering is (numbering starts with 0 though):\n",
        "        #\n",
        "        #      [      1      2      3   ...     1*nlon\n",
        "        #        nlon+1 nlon+2 nlon+3   ...     2*nlon\n",
        "        #           ...    ...    ...   ...     ...\n",
        "        #           ...    ...    ...   ...  nlat*nlon ]\n",
        "        #\n",
        "        # --> Making sure shape of lat/lon fields is like that\n",
        "        #\n",
        "\n",
        "        if np.all(np.array(lon_dims) == dimname[::1]):\n",
        "            lon = np.transpose(lon)\n",
        "            print('   >>> switched order of dimensions for variable \"{0}\"'.format(varname[0]))\n",
        "        elif np.all(np.array(lon_dims) == dimname[::-1]):\n",
        "            print('   >>> order of dimensions correct for variable \"{0}\"'.format(varname[0]))\n",
        "        else:\n",
        "            print('   >>> Dimensions found {0} does not match the dimension names specified with (-d): {1}'.format(lon_dims,dimname))\n",
        "            raise ValueError('STOP')\n",
        "\n",
        "        if np.all(np.array(lat_dims) == dimname[::1]):\n",
        "            lat = np.transpose(lat)\n",
        "            print('   >>> switched order of dimensions for variable \"{0}\"'.format(varname[1]))\n",
        "        elif np.all(np.array(lat_dims) == dimname[::-1]):\n",
        "            print('   >>> order of dimensions correct for variable \"{0}\"'.format(varname[1]))\n",
        "        else:\n",
        "            print('   >>> Dimensions found {0} does not match the dimension names specified with (-d): {1}'.format(lat_dims,dimname))\n",
        "            raise ValueError('STOP')\n",
        "\n",
        "    else:\n",
        "\n",
        "        raise ValueError(\n",
        "            \"The coord variables must have the same number of dimensions (either 1 or 2)\"\n",
        "        )\n",
        "\n",
        "    lath, lonh    = create_gridcells_from_centers(lat, lon)\n",
        "\n",
        "    nlon       = np.shape(lon)[1]\n",
        "    nlat       = np.shape(lat)[0]\n",
        "    nshapes    = nlon * nlat\n",
        "\n",
        "elif ( Path(input_file).suffix == '.shp'):\n",
        "\n",
        "    # -------------------------------\n",
        "    # Read Shapefile\n",
        "    # -------------------------------\n",
        "    print(' ')\n",
        "    print('   (1) Reading Shapefile (grid) data ...')\n",
        "\n",
        "    model_grid_shp     = gpd.read_file(input_file)\n",
        "    model_grid_shp     = model_grid_shp.to_crs(epsg=crs_caea)           # WGS 84 / North Pole LAEA Canada\n",
        "\n",
        "    nshapes    = model_grid_shp.geometry.count()    # number of shapes in model \"discretization\" shapefile (i.e. model grid-cells; not basin-discretization shapefile)\n",
        "    nlon       = 1        # only for consistency\n",
        "    nlat       = nshapes  # only for consistency\n",
        "\n",
        "else:\n",
        "\n",
        "    print(\"File extension found: {}\".format(input_file.split('.')[-1]))\n",
        "    raise ValueError('Input file needs to be either NetCDF (*.nc) or a Shapefile (*.shp).')\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Read Basin shapes and all subbasin-shapes (from toolbox)\n",
        "# -------------------------------\n",
        "print(' ')\n",
        "print('   (2) Reading shapefile data ...')\n",
        "\n",
        "shape     = gpd.read_file(routinginfo)\n",
        "# shape     = shape.to_crs(epsg=crs_lldeg)        # this is lat/lon in degree\n",
        "shape     = shape.to_crs(epsg=crs_caea)           # WGS 84 / North Pole LAEA Canada\n",
        "\n",
        "# check that key column contains only unique values\n",
        "keys = np.array(list(shape[key_colname]))\n",
        "# keys_uniq = np.unique(keys)\n",
        "# if len(keys_uniq) != len(keys):\n",
        "#     raise ValueError(\"The attribute of the shapefile set to contain only unique identifiers ('{}') does contain duplicate keys. Please specify another column (option -c '<col_name>') and use the option to process all records contained in the shapefile (-a).\".format(key_colname))\n",
        "\n",
        "\n",
        "# select only relevant basins/sub-basins\n",
        "if not(doall):\n",
        "\n",
        "    if not(basin is None):    # if gauge ID is given\n",
        "\n",
        "        basins     = [ bb.strip() for bb in basin.split(',') ]\n",
        "        idx_basins = [ list(np.where(shape['Obs_NM']==bb)[0]) for bb in basins ]\n",
        "\n",
        "        # find corresponding SubId\n",
        "        SubId = [np.int(shape.loc[idx_basin].SubId) for idx_basin in idx_basins]\n",
        "        print(\"   >>> found gauge at SubId = \",SubId)\n",
        "\n",
        "    if not(SubId is None): # if basin ID is given\n",
        "\n",
        "        old_SubIds = []\n",
        "        for SI in SubId:\n",
        "\n",
        "            old_SubId     = []\n",
        "            new_SubId     = [ SI ]\n",
        "\n",
        "            while len(new_SubId) > 0:\n",
        "\n",
        "                old_SubId.append(new_SubId)\n",
        "                new_SubId = [ list(shape.loc[(np.where(shape['DowSubId']==ii))[0]].SubId) for ii in new_SubId ]  # find all upstream catchments of these new basins\n",
        "                new_SubId = list(np.unique([item for sublist in new_SubId for item in sublist])) # flatten list and make entries unique\n",
        "\n",
        "            old_SubId   = np.array([item for sublist in old_SubId for item in sublist],dtype=np.int)  # flatten list\n",
        "            old_SubIds += list(old_SubId)\n",
        "\n",
        "        old_SubIds = list( np.sort(np.unique(old_SubIds)) )\n",
        "\n",
        "        idx_basins = [ list(np.where(shape['SubId']==oo)[0]) for oo in old_SubIds ]\n",
        "        idx_basins = [ item for sublist in idx_basins for item in sublist ]  # flatten list\n",
        "        idx_basins = list(np.unique(idx_basins))                             # getting only unique list indexes\n",
        "\n",
        "else: # all HRUs to be processed\n",
        "\n",
        "    idx_basins = list(np.arange(0,len(shape)))\n",
        "\n",
        "\n",
        "# make sure HRUs are only once in this list\n",
        "hrus = np.array( shape.loc[idx_basins][key_colname] ) #[sort_idx]\n",
        "\n",
        "idx_basins_unique = []\n",
        "hrus_unique       = []\n",
        "for ihru,hru in enumerate(hrus):\n",
        "\n",
        "    if not( hru in hrus_unique ):\n",
        "\n",
        "        hrus_unique.append(hrus[ihru])\n",
        "        idx_basins_unique.append(idx_basins[ihru])\n",
        "\n",
        "idx_basins = idx_basins_unique\n",
        "hrus       = hrus_unique\n",
        "\n",
        "\n",
        "# order according to values in \"key_colname\"; just to make sure outputs will be sorted in the end\n",
        "sort_idx = np.argsort(shape.loc[idx_basins][key_colname])\n",
        "print('   >>> HRU_IDs found = ',list(np.array( shape.loc[idx_basins][key_colname] )[sort_idx]),'  (total: ',len(idx_basins),')')\n",
        "\n",
        "# reduce the shapefile dataset now to only what we will need\n",
        "shape     = shape.loc[np.array(idx_basins)[sort_idx]]\n",
        "\n",
        "# indexes of all lines in df\n",
        "keys       = shape.index\n",
        "nsubbasins = len(keys)\n",
        "\n",
        "# initialize\n",
        "coord_catch_wkt = {}\n",
        "\n",
        "# loop over all subbasins and transform coordinates into equal-area projection\n",
        "for kk in keys:\n",
        "\n",
        "    ibasin = shape.loc[kk]\n",
        "\n",
        "    poly                = ibasin.geometry\n",
        "    try:\n",
        "        coord_catch_wkt[kk] = ogr.CreateGeometryFromWkt(poly.to_wkt())\n",
        "    except:\n",
        "        coord_catch_wkt[kk] = ogr.CreateGeometryFromWkt(poly.wkt)\n",
        "\n",
        "# -------------------------------\n",
        "# construct all grid cell polygons\n",
        "# -------------------------------\n",
        "if ( Path(input_file).suffix == '.nc'):\n",
        "\n",
        "    print(' ')\n",
        "    print('   (3) Generate shapes for NetCDF grid cells ...')\n",
        "\n",
        "    grid_cell_geom_gpd_wkt_ea = [ [ [] for ilon in range(nlon) ] for ilat in range(nlat) ]\n",
        "    if dojson:\n",
        "        grid_cell_geom_gpd_wkt_ll = [ [ [] for ilon in range(nlon) ] for ilat in range(nlat) ]\n",
        "    for ilat in range(nlat):\n",
        "        if ilat%10 == 0:\n",
        "            print('   >>> Latitudes done: {0} of {1}'.format(ilat,nlat))\n",
        "\n",
        "        for ilon in range(nlon):\n",
        "\n",
        "            # -------------------------\n",
        "            # EPSG:3035   needs a swap before and after transform ...\n",
        "            # -------------------------\n",
        "            # gridcell_edges = [ [lath[ilat,ilon]    , lonh[ilat,  ilon]    ],            # for some reason need to switch lat/lon that transform works\n",
        "            #                    [lath[ilat+1,ilon]  , lonh[ilat+1,ilon]    ],\n",
        "            #                    [lath[ilat+1,ilon+1], lonh[ilat+1,ilon+1]  ],\n",
        "            #                    [lath[ilat,ilon+1]  , lonh[ilat,  ilon+1]  ]]\n",
        "\n",
        "            # tmp = shape_to_geometry(gridcell_edges, epsg=crs_caea)\n",
        "            # tmp.SwapXY()              # switch lat/lon back\n",
        "            # grid_cell_geom_gpd_wkt_ea[ilat][ilon] = tmp\n",
        "\n",
        "            # -------------------------\n",
        "            # EPSG:3573   does not need a swap after transform ... and is much faster than transform with EPSG:3035\n",
        "            # -------------------------\n",
        "            #\n",
        "            # Windows            Python 3.8.5 GDAL 3.1.3 --> lat/lon (Ming)\n",
        "            # MacOS 10.15.6      Python 3.8.5 GDAL 3.1.3 --> lat/lon (Julie)\n",
        "            # Graham             Python 3.8.2 GDAL 3.0.4 --> lat/lon (Julie)\n",
        "            # Graham             Python 3.6.3 GDAL 2.2.1 --> lon/lat (Julie)\n",
        "            # Ubuntu 18.04.2 LTS Python 3.6.8 GDAL 2.2.3 --> lon/lat (Etienne)\n",
        "            #\n",
        "            if osgeo_version < '3.0':\n",
        "                gridcell_edges = [ [lonh[ilat,  ilon]   , lath[ilat,ilon]      ],            # for some reason need to switch lat/lon that transform works\n",
        "                                   [lonh[ilat+1,ilon]   , lath[ilat+1,ilon]    ],\n",
        "                                   [lonh[ilat+1,ilon+1] , lath[ilat+1,ilon+1]  ],\n",
        "                                   [lonh[ilat,  ilon+1] , lath[ilat,ilon+1]    ]]\n",
        "            else:\n",
        "                gridcell_edges = [ [lath[ilat,ilon]     , lonh[ilat,  ilon]    ],            # for some reason lat/lon order works\n",
        "                                   [lath[ilat+1,ilon]   , lonh[ilat+1,ilon]    ],\n",
        "                                   [lath[ilat+1,ilon+1] , lonh[ilat+1,ilon+1]  ],\n",
        "                                   [lath[ilat,ilon+1]   , lonh[ilat,  ilon+1]  ]]\n",
        "\n",
        "            tmp = shape_to_geometry(gridcell_edges, epsg=crs_caea)\n",
        "            grid_cell_geom_gpd_wkt_ea[ilat][ilon] = tmp\n",
        "\n",
        "            if dojson:\n",
        "                tmp = shape_to_geometry(gridcell_edges)\n",
        "                grid_cell_geom_gpd_wkt_ll[ilat][ilon] = tmp\n",
        "\n",
        "elif ( Path(input_file).suffix == '.shp'):\n",
        "\n",
        "    # -------------------------------\n",
        "    # Grid-cells are actually polygons in a shapefile\n",
        "    # -------------------------------\n",
        "    print(' ')\n",
        "    print('   (3) Extract shapes from shapefile ...')\n",
        "\n",
        "    grid_cell_geom_gpd_wkt_ea = [ [ [] for ilon in range(nlon) ] for ilat in range(nlat) ]   # nlat = nshapes, nlon = 1\n",
        "    for ishape in range(nshapes):\n",
        "\n",
        "        idx = np.where(model_grid_shp[key_colname_model] == ishape)[0]\n",
        "        if len(idx) == 0:\n",
        "            print(\"Polygon ID = {} not found in '{}'. Numbering of shapefile attribute '{}' needs to be [0 ... {}-1].\".format(ishape,input_file,key_colname_model,nshapes))\n",
        "            raise ValueError('Polygon ID not found.')\n",
        "        if len(idx) > 1:\n",
        "            print(\"Polygon ID = {} found multiple times in '{}' but needs to be unique. Numbering of shapefile attribute '{}' needs to be [0 ... {}-1].\".format(ishape,input_file,key_colname_model,nshapes))\n",
        "            raise ValueError('Polygon ID not unique.')\n",
        "        idx  = idx[0]\n",
        "        poly = model_grid_shp.loc[idx].geometry\n",
        "        try:\n",
        "            grid_cell_geom_gpd_wkt_ea[ishape][0] = ogr.CreateGeometryFromWkt(poly.to_wkt())\n",
        "        except:\n",
        "            grid_cell_geom_gpd_wkt_ea[ishape][0] = ogr.CreateGeometryFromWkt(poly.wkt)\n",
        "\n",
        "else:\n",
        "\n",
        "    print(\"File extension found: {}\".format(input_file.split('.')[-1]))\n",
        "    raise ValueError('Input file needs to be either NetCDF (*.nc) or a Shapefile (*.shp).')\n",
        "\n",
        "# -------------------------------\n",
        "# Derive overlay and calculate weights\n",
        "# -------------------------------\n",
        "print(' ')\n",
        "print('   (4) Deriving weights ...')\n",
        "\n",
        "filename = output_file\n",
        "ff       = open(filename,'w')\n",
        "ff.write(':GridWeights                     \\n')\n",
        "ff.write('   #                                \\n')\n",
        "ff.write('   # [# HRUs]                       \\n')\n",
        "ff.write('   :NumberHRUs       {0}            \\n'.format(nsubbasins))\n",
        "ff.write('   :NumberGridCells  {0}            \\n'.format(nshapes))\n",
        "ff.write('   #                                \\n')\n",
        "ff.write('   # [HRU ID] [Cell #] [w_kl]       \\n')\n",
        "\n",
        "if dojson:\n",
        "    cells_to_write_to_geojson = []\n",
        "    geojson = []\n",
        "\n",
        "error_dict = {}\n",
        "for ikk,kk in enumerate(keys):\n",
        "\n",
        "    ibasin = shape.loc[kk]\n",
        "\n",
        "    area_basin = coord_catch_wkt[kk].Area()\n",
        "    enve_basin = coord_catch_wkt[kk].GetEnvelope()   # bounding box around basin (for easy check of proximity)\n",
        "\n",
        "    area_all = 0.0\n",
        "    ncells   = 0\n",
        "\n",
        "    data_to_write = []\n",
        "    for ilat in range(nlat):\n",
        "        for ilon in range(nlon):\n",
        "\n",
        "            enve_gridcell  = grid_cell_geom_gpd_wkt_ea[ilat][ilon].GetEnvelope()   # bounding box around grid-cell (for easy check of proximity)\n",
        "            grid_is_close  = check_proximity_of_envelops(enve_gridcell, enve_basin)\n",
        "\n",
        "            if grid_is_close: # this check decreases runtime DRASTICALLY (from ~6h to ~1min)\n",
        "\n",
        "                grid_cell_area = grid_cell_geom_gpd_wkt_ea[ilat][ilon].Area()\n",
        "\n",
        "                inter = (grid_cell_geom_gpd_wkt_ea[ilat][ilon].Buffer(0.0)).Intersection(coord_catch_wkt[kk].Buffer(0.0)) # \"fake\" buffer to avoid invalid polygons and weirdos dumped by ArcGIS\n",
        "                area_intersect = inter.Area()\n",
        "\n",
        "                area_all += area_intersect\n",
        "                if area_intersect > 0:\n",
        "\n",
        "                    ncells += 1\n",
        "                    cell_ID = ilat*nlon+ilon\n",
        "                    data_to_write.append( [int(ibasin[key_colname]),ilat,ilon,cell_ID,area_intersect/area_basin] )\n",
        "\n",
        "                    if dojson:\n",
        "                        if cell_ID not in cells_to_write_to_geojson:\n",
        "                            cells_to_write_to_geojson.append( cell_ID )\n",
        "                            tmp = grid_cell_geom_gpd_wkt_ll[ilat][ilon].Buffer(0.0).ExportToJson()\n",
        "                            geojson.append({\"type\":\"Feature\",\"geometry\":gjs.loads(tmp),\"properties\":{\"CellId\":cell_ID,\"Area\":grid_cell_area}})\n",
        "\n",
        "    # mismatch between area of subbasin (shapefile) and sum of all contributions of grid cells (model output)\n",
        "    error = (area_basin - area_all)/area_basin\n",
        "\n",
        "\n",
        "    if abs(error) > area_error_threshold and area_basin > 500000.:\n",
        "        # record all basins with errors larger 5% (if basin is larger than 0.5 km2)\n",
        "        error_dict[int(ibasin[key_colname])] = [ error, area_basin ]\n",
        "        for idata in data_to_write:\n",
        "            print(\"   >>> {0},{1},{2},{3},{4}\".format(idata[0],idata[1],idata[2],idata[3],idata[4]))\n",
        "            ff.write(\"   {0}   {1}   {2}\\n\".format(idata[0],idata[3],idata[4]))\n",
        "\n",
        "    else:\n",
        "        # adjust such that weights sum up to 1.0\n",
        "        for idata in data_to_write:\n",
        "            corrected = idata[4] * 1.0/(1.0-error)\n",
        "            print(\"   >>> {0},{1},{2},{3},{4}  (corrected to {5})\".format(idata[0],idata[1],idata[2],idata[3],idata[4],corrected))\n",
        "            ff.write(\"   {0}   {1}   {2}\\n\".format(idata[0],idata[3],corrected))\n",
        "\n",
        "        if error < 1.0:\n",
        "            area_all *= 1.0/(1.0-error)\n",
        "        error    = 0.0\n",
        "\n",
        "    print('   >>> (Sub-)Basin: {0} ({1} of {2})'.format(int(ibasin[key_colname]),ikk+1,nsubbasins))\n",
        "    print('   >>> Derived area of {0}  cells: {1}'.format(ncells,area_all))\n",
        "    print('   >>> Read area from shapefile:   {0}'.format(area_basin))\n",
        "    print('   >>> error:                      {0}%'.format(error*100.))\n",
        "    print('   ')\n",
        "\n",
        "ff.write(':EndGridWeights \\n')\n",
        "ff.close()\n",
        "\n",
        "# write geoson\n",
        "if dojson:\n",
        "\n",
        "    json_file = '.'.join(filename.split('.')[0:-1])+\".json\"\n",
        "\n",
        "    geojson = {\"type\":\"FeatureCollection\",\"features\":geojson}\n",
        "    with open(json_file, 'w') as outfile:\n",
        "      gjs.dump(geojson, outfile)\n",
        "\n",
        "\n",
        "\n",
        "# print out all subbasins that have large errors\n",
        "if (error_dict != {}):\n",
        "    print('')\n",
        "    print('WARNING :: The following (sub-)basins show large mismatches between provided model')\n",
        "    print('           grid and domains spefied in the shapefile. It seems that your model')\n",
        "    print('           output is not covering the entire domain!')\n",
        "    print(\"           { <basin-ID>: <error>, ... } = \")\n",
        "    for attribute, value in error_dict.items():\n",
        "        print('               {0} : {1:6.2f} % of {2:8.1f} km2 basin'.format(attribute, value[0]*100., value[1]/1000./1000.))\n",
        "    print('')\n",
        "\n",
        "print('')\n",
        "print('Wrote: ',filename)\n",
        "if dojson:\n",
        "    print('Wrote: ',json_file)\n",
        "print('')"
      ],
      "metadata": {
        "id": "Jv4L9rnPgOkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post-processing to match HRUs used in model to ones in gridded weights"
      ],
      "metadata": {
        "id": "s9TJPlLC0b2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# converted gridded weights to csv file\n",
        "data = pd.read_csv('/content/GridWeights.csv')"
      ],
      "metadata": {
        "id": "Meo4mb6t01Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manually identified the HRU IDs present within the RVH file\n",
        "hru_rvh_ids = [1,2,3,4,5,8,9,15,16,20,21,22,23,24,28,29,34,37,39,40,44,45,52,53,67,70,75,76,77,82,87,97,99,100,101,105,107,109,110,116,117,118,120,126,131,133,136,137,138,139,140,141,146,148,150,152,153,154,155,156,158,159,161,163,166,167,201,207,208,215,233,234,235,236,237,238,239,240,251,257,260,261,264,271,272,275,276,277,278,279,280,281,298,302,307,308,309,310,312,314,315,318,319,320,321,322,325,330,335,336,337,338,339,340,341,343,344,350,351,352,358,359,364,366,368,369,370,371,372,373,374,379,384,386,387,388,389,390,391,394,399,400,401,402,403,404,405,407,408,410,411,412,413,414,415,416,417,418,419,420,423,424,426,428,433,436,437,438,441,442,444,447,448,449,450,453,454,455,456,457,458,461,462,465,466,467,469,470,476,478,479,480,481,487,488,494,495,498,499,500,501,505,506,507,510,511,515,518,519,525,526,528,535,545,548,549,550,551,554,555,556,558,559,560,561,563,564,565,567,573,577,578,579,584,585,587,588,589,590,591,592,596,597,598,600,601,604,606,607,608,609,610,611,627,628,629,636,638,639,642,644,645,649,650,651,661,662,688,689,690,691,693,694,695,696,703,704,709,710,711,712,713,717,722,723,724,725,726,727,728,729,730,738,741,742,743,744,745,746,747,748,749,754,755,756,757,758,760,761,763,764,765,766,767,769,770,776,777,778,780,781,782,783,784,785,786,787,790,791,792,793,794,795,801,802,803,804,805,806,807,808,811,812,813,814,816,818,829,830,835,836,837,838,843,844,845,846,848,851,860,861,867,868,869,872,873,876,877,878,879,880,881,896,897,900,905,906,907,909,910,911,912,913,914,918,920,921,922,924,927,928,932,936,938,941,942,943,944,947,948,949,951,952,956,970,971,972,975,976,977,983,984,985,986,989,990,993,994,998,999,1000,1007,1009,1010,1011,1013,1017,1024,1025,1031,1032,1034,1036,1037,1038,1041,1046,1047,1049,1051,1063,1064,1066,1067,1068,1069,1082,1085,1086,1087,1089,1094,1095,1096,1104,1105,1106,1109,1110,1124,1125,1126,1127,1128,1129,1130,1131,1133,1134,1137,1142,1143,1144,1146,1149,1150,1151,1158,1160,1161,1162,1163,1170,1171,1177,1182,1190,1191,1192,1193,1194,1195,1198,1209,1212,1213,1214,1216,1220,1221,1222,1223,1224,1226,1227,1230,1238,1239,1240,1242,1256,1257,1264,1265,1266,1268,1270,1271,1272,1275,1278,1279,1280,1282,1283,1291,1301,1302,1303,1304,1305,1306,1307,1308,1309,1311,1313,1314,1332,1333,1334,1336,1338,1343,1345,1347,1350,1351,1352,1359,1365,1368,1370,1372,1373,1377,1381,1382,1386,1387,1392,1395,1396,1397,1403,1404,1405,1410,1411,1413,1415,1416,1418,1419,1422,1427,1432,1437,1439,1440,1441,1443,1445,1446,1448,1449,1451,1453,1458,1459,1465,1466,1467,1469,1470,1480,1481,1484,1485,1487,1489,1494,1497,1498,1500,1501,1509,1513,1514,1515,1518,1520,1521,1522,1526,1527,1530,1541,1542,1543,1544,1545,1551,1555,1559,1562,1563,1582,1583,1584,1586,1587,1591,1602,1603,1605,1606,1607,1609,1610,1611,1612,1613,1614,1618,1619,1622,1623,1625,1633,1634,1638,1640,1641,1649,1654,1655,1656,1662,1663,1664,1665,1666,1672,1673,1674,1677,1678,1681,1686,1687,1688,1690,1697,1699,1700,1702,1703,1704,1707,1709,1710,1711,1713,1714,1716,1717,1718,1719,1721,1722,1727,1728,1729,1730,1737,1738,1743,1744,1745,1746,1748,1755,1758,1761,1762,1763,1766,1767,1768,1769,1772,1775,1776,1778,1783,1784,1785,1790,1792,1801,1806,1808,1811,1812,1815,1820,1824,1825,1828,1829,1832,1844,1845,1846,1848,1850,1854,1856,1858,1861,1863,1868,1869,1877,1878,1879,1880,1883,1884,1888,1889,1892,1893,1894,1897,1899,1900,1901,1906,1907,1908,1911,1913,1915,1916,1921,1923,1925,1926,1929,1930,1933,1934,1946,1947,1948,1949,1951,1952,1958,1959,1960,1961,1963,1966,1972,1973,1975,1976,1977,1979,1981,1986,1987,1988,1989,1991,1995,1997,1999,2001,2002,2003,2004,2008,2011,2012,2014,2015,2016,2018,2019,2020,2022,2023,2026,2028,2032,2033,2035,2036,2038,2039,2040,2041,2042,2045,2057,2058,2059,2060,2061,2065,2066,2067,2068,2072,2073,2083,2087,2089,2091,2092,2094,2095,2096,2097,2099,2101,2107,2108,2109,2111,2112,2113,2115,2116,2127,2131,2133,2134,2135,2136,2138,2142,2145,2148,2149,2150,2151,2152,2153,2154,2155,2158,2159,2160,2161,2168,2171,2172,2174,2178,2179,2181,2185,2186,2187,2188,2191,2192,2193,2194,2195,2196,2197,2198,2204,2205,2206,2209,2211,2212,2213,2214,2215,2216,2217,2218,2219,2220,2223,2224,2225,2226,2227,2229,2230,2231,2232,2233,2235,2236,2238,2239,2243,2244,2245,2246,2247,2248,2249,2250,2253,2254,2255,2257,2258,2259,2260,2261,2262,2263,2265,2266,2267,2268,2269,2270,2271,2273,2274,2275,2280,2281,2282,2283,2284,2285,2288,2304,2307,2308,2312,2326,2327,2330,2331,2332,2333,2334,2335,2336,2337,2338,2339,2340,2341,2342]"
      ],
      "metadata": {
        "id": "OewF4gM-y3T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop through csv dataframe and select HRUs that are present within the RVH file\n",
        "grid_vals_rvh = data[data['HRU_ID'].isin(hru_rvh_ids)]\n",
        "#print(grid_vals_rvh)\n",
        "display(grid_vals_rvh)"
      ],
      "metadata": {
        "id": "JvdgR5-H06zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write new gridded weights with only RVH HRUs\n",
        "num_hrus = grid_vals_rvh['HRU_ID'].nunique()\n",
        "num_gridCell = 1938   # value from previous GridWeight.txt files\n",
        "\n",
        "lst_vals1 = list(range(0, len(grid_vals_rvh['HRU_ID'])))\n",
        "\n",
        "hru_id = ''.join(grid_vals_rvh.to_string(index=False, header=False))\n",
        "#hru_id = ''.join(grid_vals_rvh.to_string(index=False, header=False))\n",
        "#cell_num = '\\n\\t\\t'.join(grid_vals_rvh['Cell_num'].astype(str).apply(lambda x: ''.join(x)))\n",
        "#w_kl = '\\t'.join(grid_vals_rvh['w_kl'].astype(str).apply(lambda x: '  '.join(x)))\n",
        "\n",
        "f = open('/content/GridWeights.txt', \"a\")\n",
        "print(f\":GridWeights\", file=f)\n",
        "print(f\"\\t#\", file=f)\n",
        "print(f\"\\t# [# HRUs]\", file=f)\n",
        "print(f\"\\t:NumberHRUs       {num_hrus}\", file=f)\n",
        "print(f\"\\t:NumberGridCells  {num_gridCell}\", file=f)\n",
        "print(f\"\\t#\", file=f)\n",
        "print(f\"\\t# [HRU ID] [Cell #] [w_kl]\", file=f)\n",
        "print(f\"\\t {hru_id}\", file=f)\n",
        "print(f\":EndGridWeights \", file=f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "421T8z244aG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iRvQvyONa6zW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}