{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# if using Google Colab - to connect to drive\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/google_drive')"
      ],
      "metadata": {
        "id": "DAIq_O1NGkUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9WRPevFE_GG"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install cdo &> /dev/null\n",
        "\n",
        "!pip install pyshp &> /dev/null\n",
        "!pip install geopandas==0.13.0 &> /dev/null\n",
        "#!pip install cftime &> /dev/null\n",
        "#!pip install netcdf4 &> /dev/null\n",
        "\n",
        "# load in libraries\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shapefile\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import mapping\n",
        "import shutil\n",
        "import zipfile\n",
        "from glob import glob\n",
        "import xarray as xr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define paths\n",
        "main_dir = \"/content\"\n",
        "folder_name = \"Milk_Mary_model\"\n",
        "drive_dir = os.path.join(main_dir, 'google_drive', 'MyDrive', folder_name)"
      ],
      "metadata": {
        "id": "fjNUgC-7FY5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional** - Prepare the shapefile for CaSPAr by altering the shapefile to a single polygon geometry"
      ],
      "metadata": {
        "id": "1ZH3Dp6OHbjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up directories\n",
        "# define temporary directory\n",
        "net_dir = os.path.join(main_dir, 'data_temporary')\n",
        "net_path = os.path.isdir(net_dir)\n",
        "if not net_path:\n",
        "  os.makedirs(net_dir)\n",
        "  print(\"created  folder: \", net_dir)\n",
        "\n",
        "# define temporary shapefile directory\n",
        "shp_dir = os.path.join(main_dir, 'shapefile_temporary')\n",
        "shp_path = os.path.isdir(shp_dir)\n",
        "if not shp_path:\n",
        "  os.makedirs(shp_dir)\n",
        "  print(\"created  folder: \", shp_dir)"
      ],
      "metadata": {
        "id": "_GTw12dzFHXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# format shapefile by removing multipolygons\n",
        "# Copyright 2016-2020 Juliane Mai - juliane.mai(at)uwaterloo.ca\n",
        "# License\n",
        "# This file is part of Juliane Mai's personal code library.\n",
        "from __future__ import print_function\n",
        "def coords2shapefile(filename,coords):\n",
        "\n",
        "    # make sure coords is a list of lists\n",
        "    coords = [ list(ii) for ii in coords ]\n",
        "\n",
        "    # -----------------------\n",
        "    # Check if polygon is clockwise:\n",
        "    #       Use \"shapefile.signed_area()\" method to determine if a ring is clockwise or counter-clockwise\n",
        "    #       Value >= 0 means the ring is counter-clockwise.\n",
        "    #       Value <  0 means the ring is clockwise\n",
        "    #       The value returned is also the area of the polygon.\n",
        "    # -----------------------\n",
        "    area = shapefile.signed_area(coords)\n",
        "\n",
        "    if area >= 0:\n",
        "        coords.reverse()    # transform counter-clockwise to clockwise\n",
        "\n",
        "    if sys.version_info < (3,0,0):\n",
        "        # ------------------------\n",
        "        # Create a polygon shapefile\n",
        "        # ------------------------\n",
        "        # Found under:\n",
        "        #     https://code.google.com/archive/p/pyshp/\n",
        "        w = shapefile.Writer(shapefile.POLYGON)\n",
        "\n",
        "        # an arrow-shaped polygon east of Vancouver, Seattle, and Portland\n",
        "        w.poly([coords])\n",
        "        w.field('FIRST_FLD','C','40')\n",
        "        w.record('First','Polygon')\n",
        "        w.save(filename)\n",
        "    else:\n",
        "        # ------------------------\n",
        "        # Create a polygon shapefile\n",
        "        # ------------------------\n",
        "        # Found under:\n",
        "        #     https://code.google.com/archive/p/pyshp/\n",
        "        w = shapefile.Writer(target=filename)\n",
        "\n",
        "        # an arrow-shaped polygon east of Vancouver, Seattle, and Portland\n",
        "        w.poly([coords])\n",
        "        w.field('FIRST_FLD','C','40')\n",
        "        w.record('First','Polygon')\n",
        "        w.close()\n",
        "\n",
        "\n",
        "    # ------------------------\n",
        "    # Write projection information\n",
        "    # ------------------------\n",
        "    # Found under:\n",
        "    #     https://code.google.com/archive/p/pyshp/wikis/CreatePRJfiles.wiki\n",
        "    prj = open(\"%s.prj\" % filename, \"w\")\n",
        "    epsg = 'GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433]]'\n",
        "    prj.write(epsg)\n",
        "    prj.close()\n",
        "\n",
        "    return\n",
        "\n",
        "# read in shapefile and check geometry\n",
        "\"\"\"for shp_file in os.listdir(os.path.join(drive_dir, 'shapefile')):    # finds shapefile name for you, however only useful if there is only one shapefile in folder\n",
        "    if shp_file.endswith(\".shp\"):\n",
        "      shp_file_name = shp_file\n",
        "shp_file = gpd.read_file(os.path.join(drive_dir, 'shapefile', shp_file_name))\"\"\"\n",
        "\n",
        "shp_file = gpd.read_file(os.path.join(drive_dir, 'shapefile', 'studyArea_outline.shp'))\n",
        "\n",
        "# format shapefile if not a singular polygon\n",
        "explode = shp_file.explode()\n",
        "num_poly = explode['geometry'].count()\n",
        "\n",
        "if num_poly > 1:\n",
        "  g = [i for i in shp_file.geometry]\n",
        "  geojson_ob = mapping(g[0]) # for first feature/row\n",
        "  all_coords = geojson_ob[\"coordinates\"]\n",
        "\n",
        "  print(all_coords)\n",
        "\n",
        "  flat_list = []\n",
        "\n",
        "  for sublist in all_coords[0]:\n",
        "      for item in sublist:\n",
        "          flat_list.append(item)\n",
        "\n",
        "  if __name__ == '__main__':\n",
        "    # import doctest\n",
        "    # doctest.testmod(optionflags=doctest.NORMALIZE_WHITESPACE)\n",
        "    coords = flat_list\n",
        "    filename = os.path.join(shp_dir,'polygon')   # don't use any file ending here\n",
        "\n",
        "    coords2shapefile(filename,coords)\n",
        "\n",
        "    shp_check = gpd.read_file(os.path.join(shp_dir,'polygon.shp'))\n",
        "    shp_check.plot()\n",
        "\n",
        "else:\n",
        "  shp_file = gpd.read_file(os.path.join(drive_dir, 'shapefile', 'studyArea_outline.shp'))    # may need to adjust shapefile name\n",
        "  shp_file.to_file(os.path.join(shp_dir, 'study_area.geojson'), driver='GeoJSON')\n",
        "  shp_file.plot()"
      ],
      "metadata": {
        "id": "famkUStWFHZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zip folder\n",
        "zip_name = os.path.join(shp_dir)\n",
        "directory_name = os.path.join(main_dir, 'shapefile')\n",
        "print('Folder zipped')\n",
        "\n",
        "# Create 'path\\to\\zip_file.zip'\n",
        "shutil.make_archive(directory_name, 'zip', zip_name)"
      ],
      "metadata": {
        "id": "UbFQ0HjtFHcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right click on the zipped folder and select \"Download\" to save to your local computer\n",
        "\n",
        "Once the zipped folder has been downloaded to your local computer, please click on the following link: [CaSPAr Data Portal](https://caspar-data.ca/caspar)\n",
        "\n",
        "Need a Globus ID, no problem! Click [here](https://www.globusid.org/) for more information\n",
        "\n",
        "If you are new to Globus you will also need to install Globus Connect Personal in order to transer the CaSPAr data to your local machine, the link to download the software is available [here](https://www.globus.org/globus-connect-personal)\n"
      ],
      "metadata": {
        "id": "I1HF9fYKG1nK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define new directories\n",
        "# define temporary directory\n",
        "net_temp_dir = os.path.join(main_dir, 'data_temporary') # path to where the CaSPAr data is saved\n",
        "net_temp_path = os.path.isdir(net_temp_dir)\n",
        "if not net_path:\n",
        "  os.makedirs(net_temp_dir)\n",
        "  print(\"created  folder: \", net_temp_dir)\n",
        "\n",
        "# define bash script directory\n",
        "bash_dir = os.path.join(main_dir, 'data_temporary','bash_scripts')\n",
        "bash_path = os.path.isdir(bash_dir)\n",
        "if not bash_path:\n",
        "  os.makedirs(bash_dir)\n",
        "  print(\"created  folder: \", bash_dir)\n",
        "\n",
        "# define CaSPAr output directory\n",
        "caspar_dir = os.path.join(drive_dir, 'workflow_outputs','RavenInput', 'input')\n",
        "caspar_path = os.path.isdir(caspar_dir)\n",
        "if not caspar_path:\n",
        "  os.makedirs(caspar_dir)\n",
        "  print(\"created  folder: \", caspar_dir)"
      ],
      "metadata": {
        "id": "YBBK8txYG0hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip CaSPAr data\n",
        "extension = \".zip\"\n",
        "\n",
        "os.chdir(net_dir) # change directory from working dir to dir with files\n",
        "\n",
        "for item in os.listdir(net_temp_dir): # loop through items in dir\n",
        "    if item.endswith(extension): # check for \".zip\" extension\n",
        "        file_name = os.path.abspath(item) # get full path of files\n",
        "        zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n",
        "        zip_ref.extractall(net_dir) # extract file to dir\n",
        "        zip_ref.close() # close file\n",
        "        os.remove(file_name) # delete zipped"
      ],
      "metadata": {
        "id": "Fuq2x6NqI1Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define merged CaSPAr data name\n",
        "# note - do NOT add .nc on the end of the merged CaSPAr name\n",
        "merged_Casp_name = 'RDRSv2_hourly'"
      ],
      "metadata": {
        "id": "PtkLfxbaJKEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overview of the the NetCDF files thus far\n",
        "configfiles = glob('/content/data_temporary/*/*.nc') # may need to adjust path, depending where data is saved\n",
        "test = xr.open_dataset(configfiles[0])\n",
        "print(test)\n",
        "#display(test)      # if in Jupyter Notebook environment"
      ],
      "metadata": {
        "id": "zhxuSzfBJa0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge NetCDF files\n",
        "# define path of CaSPAr file, do not include the name of the files\n",
        "# for example, /content/data_temporary/data/\n",
        "\n",
        "merged_Casp_name = 'caspar_data'\n",
        "\n",
        "casp_file_path = '/content/data_temporary/hb_1688580282/'  # adjust path as needed\n",
        "\n",
        "casp_file = os.path.join(casp_file_path,'*.nc')\n",
        "\n",
        "#bash command\n",
        "with open(os.path.join(bash_dir,'merge.sh'), 'w') as f2:\n",
        "    print(f'cdo mergetime {casp_file} /content/data_temporary/{merged_Casp_name}.nc', file=f2) # adjust path as needed\n",
        "\n",
        "!bash /content/data_temporary/bash_scripts/merge.sh  # able to run bash directly in Jupyter notebook environment"
      ],
      "metadata": {
        "id": "TliS8CeHJ_tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adjust unit of time\n",
        "# The data products from CaSPAr are in UTC time, meaning that the data will likely\n",
        "# need to be shifted in order to align with other data in your model (such as observed flows)\n",
        "ds = xr.open_dataset('/content/data_temporary/caspar_data.nc') # read in NetCDF\n",
        "\n",
        "# Time shift to local time zone\n",
        "UTC_tz = 5      # UTC-5\n",
        "old_start_datetime = ds['time'][0].to_numpy()[()]\n",
        "new_start_datetime = old_start_datetime - np.timedelta64(UTC_tz, 'h')\n",
        "utc5time = pd.date_range(new_start_datetime, freq='1H', periods=len(ds['time']))\n",
        "ds['time'] = utc5time\n",
        "\n",
        "# save the merged netcdf file\n",
        "ds.to_netcdf('/content/data_temporary/caspar_data_adjusted.nc', 'w') # adjust path as needed"
      ],
      "metadata": {
        "id": "GTXxwEjqKaem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overview of adjusted CaSPAr data\n",
        "check_casp = xr.open_dataset(os.path.join(net_dir, f'caspar_data_adjusted.nc'))\n",
        "print(check_casp)\n",
        "# display(check_casp)  # if in Jupyter Notebook environment"
      ],
      "metadata": {
        "id": "_JfXW-YFLc_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# temporatl aggregation of hourly data to daily\n",
        "\n",
        "# PRECIPITATION -----------------------------------------------------------------------------------------------------------------------------------\n",
        "# convert from m to mm\n",
        "precip = check_casp['RDRS_v2.1_P_PR0_SFC']                                              # need to be customized by user\n",
        "precip = precip * 1000\n",
        "precip_ds = precip.to_dataset()\n",
        "# save the aggregated file\n",
        "precip_ds.to_netcdf(os.path.join(net_dir, 'prcp_mm_data.nc'), 'w')\n",
        "\n",
        "# aggregation of hourly to daily data\n",
        "# adjust path as needed\n",
        "!cdo daysum /content/data_temporary/prcp_mm_data.nc /content/google_drive/MyDrive/Milk_Mary_model/workflow_outputs/RavenInput/input/mean_prcp_daily.nc\n",
        "\n",
        "\n",
        "# TEMPERATURE -----------------------------------------------------------------------------------------------------------------------------------\n",
        "temp = check_casp['RDRS_v2.1_P_TT_1.5m']\n",
        "mean_temp_ds = temp.to_dataset()\n",
        "mean_temp_ds.to_netcdf(os.path.join(net_dir, 'temp_data.nc'), 'w')\n",
        "# aggregation of hourly to daily data\n",
        "\n",
        "# Daily Average Temperature\n",
        "# adjust path as needed\n",
        "# first path - where the input data is saved\n",
        "# second path - where the output data should be saved/named\n",
        "!cdo daymean /content/data_temporary/temp_data.nc /content/google_drive/MyDrive/Milk_Mary_model/workflow_outputs/RavenInput/input/mean_temp_daily.nc\n",
        "\n",
        "# Daily Maximum Temperature\n",
        "# adjust path as needed\n",
        "# first path - where the input data is saved\n",
        "# second path - where the output data should be saved/named\n",
        "!cdo daymax /content/data_temporary/temp_data.nc /content/google_drive/MyDrive/Milk_Mary_model/workflow_outputs/RavenInput/input/max_temp_daily.nc\n",
        "\n",
        "# Daily Minimum Temperature\n",
        "# adjust path as needed\n",
        "# first path - where the input data is saved\n",
        "# second path - where the output data should be saved/named\n",
        "!cdo daymin /content/data_temporary/temp_data.nc /content/google_drive/MyDrive/Milk_Mary_model/workflow_outputs/RavenInput/input/min_temp_daily.nc"
      ],
      "metadata": {
        "id": "fk3H8XJeLyEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check NetCDF file again (ensure they are not empty)\n",
        "check_casp = xr.open_dataset('/content/google_drive/MyDrive/Milk_Mary_model/workflow_outputs/RavenInput/input/min_temp_daily.nc')\n",
        "print(check_casp)\n",
        "#display(check_casp)  # if in Jupyter environment"
      ],
      "metadata": {
        "id": "8O4bAtNjMyiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate list of filenames for Raven RVT file\n",
        "file_names_lst = []\n",
        "files_input = glob(os.path.join(caspar_dir, \"*\")) # may need to adjust path to where the final CaSPAr files are saved\n",
        "\n",
        "for f_name in list(range(0, len(files_input))):\n",
        "  base = os.path.basename(files_input[f_name])\n",
        "  file_names_lst.append(base)\n",
        "\n",
        "print(file_names_lst)"
      ],
      "metadata": {
        "id": "kjUR2lXlMdBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate Raven RVT File** </font></br>\n",
        "\n",
        "example format of a Gridded Forcing RVT file:<br>\n",
        "<font color=grey> :GriddedForcing           Averagetemp<br>\n",
        "><font color=grey> :ForcingType            TEMP_AVE<br>\n",
        ">:FileNameNC             input/VIC_temperatures.nc<br>\n",
        ">:VarNameNC              Avg_temp<br>\n",
        ">:DimNamesNC             lon lat time<br>\n",
        ">:RedirectToFile         GridWeights.txt\n",
        "\n",
        "<font color=grey> :EndGriddedForcing"
      ],
      "metadata": {
        "id": "euoQ9UETNEVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model name (for RVT file)\n",
        "model_name = 'model_name'"
      ],
      "metadata": {
        "id": "epImktYaNMzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define forcing types\n",
        "# include quotations around each variable and square brackets\n",
        "# for example, ['Precipitation', 'Averagetemp']\n",
        "forcing_name = ['Mintemp', 'Maxtemp', 'Precipitation']"
      ],
      "metadata": {
        "id": "-Mgp875bNPRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define forcing types\n",
        "# include quotations around each variable and square brackets\n",
        "# for example, ['PRECIP', 'TEMP_AVE']\n",
        "forcing_type = ['TEMP_DAILY_MIN', 'TEMP_DAILY_MAX', 'PRECIP']"
      ],
      "metadata": {
        "id": "crwlHG6KNXSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define variable name in NetCDF file\n",
        "# include quotations around each variable and square brackets\n",
        "# for example, ['RDRS_v2.1_P_PR0_SFC', 'RDRS_v2.1_P_TT_1.5m']\n",
        "var_names = ['RDRS_v2.1_P_TT_1.5m', 'RDRS_v2.1_P_TT_1.5m', 'RDRS_v2.1_P_PR0_SFC']"
      ],
      "metadata": {
        "id": "1-mFHqcwNfOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dimension names\n",
        "# seperate with spaces and include quotations\n",
        "# for example, 'rlon rlat time'\n",
        "dim_names = 'rlon rlat time'"
      ],
      "metadata": {
        "id": "LnoCFET7NnW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using a print statement to write and save the RVT file\n",
        "with open(os.path.join(drive_dir, 'workflow_outputs','RavenInput',model_name+'.rvt'), \"a\") as f:\n",
        "  print(f\"#------------------------------------------------------------------------\", file=f)\n",
        "  print(f\"# CaSPAr Data Input File\", file=f)\n",
        "  print(f\"#------------------------------------------------------------------------\\n#\", file=f)\n",
        "\n",
        "lst_vals = list(range(0, len(forcing_type)))\n",
        "\n",
        "for n in lst_vals:\n",
        "  #n1 = forc_input[(forc_input.lon == df_coords1.lon[n])&(forc_input.lat == df_coords1.lat[n])].reset_index(drop=True)\n",
        "  #print(n1)\n",
        "  f = open(os.path.join(drive_dir, 'workflow_outputs','RavenInput',model_name+'.rvt'), \"a\")\n",
        "  print(f\":GriddedForcing \\t\\t\\t{forcing_name[n]}\", file=f)\n",
        "  print(f\"\\t:ForcingType \\t\\t\\t\\t{forcing_type[n]}\", file=f)\n",
        "  print(f\"\\t:FileNameNC \\t\\t\\t\\tinput/{file_names_lst[n]}\", file=f)\n",
        "  print(f\"\\t:VarNameNC \\t\\t\\t\\t\\t{var_names[n]}\", file=f)\n",
        "  print(f\"\\t:DimNamesNC \\t\\t\\t\\t{dim_names}\", file=f)\n",
        "  print(f\"\\t:RedirectToFile \\t\\tGridWeights.txt\", file=f)\n",
        "  print(f\":EndGriddedForcing\", file=f)\n",
        "  print(f\"#\", file=f)\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "BvqAJVpkMwCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optional** - remove temporary folders that were generated and no longer usefule\n",
        "remove_drainage_region_temporary_files = False   # change to True to delete folders\n",
        "\n",
        "# delete temorary folder\n",
        "if remove_drainage_region_temporary_files == True:\n",
        "  shutil.rmtree(os.path.join(main_dir, 'data_temporary'))"
      ],
      "metadata": {
        "id": "yePsDdGAN0xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d4KX1nnsP5bp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}